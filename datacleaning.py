# -*- coding: utf-8 -*-
"""DataCleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vnqqFtnQc9DDnXMhAQ0OESN1cDDk1wxo
"""

import pandas as pd 
import re 
from nltk.tokenize import word_tokenize

news= open('/content/drive/MyDrive/en_US/en_US.news.txt',encoding="utf8").read()
blogs= open('/content/drive/MyDrive/en_US/en_US.blogs.txt',encoding="utf8").read()

print(len(news))
print(len(blogs))

text_corpus1=news+blogs 
print(len(text_corpus1))

"""Cleaning
Removing Extra space



Removing Special Characters
Tokenizing text data
Tokenizing Twitter data
"""

def extra_space(text):
    new_text= re.sub("\s+"," ",text)
    return new_text
def sp_charac(text):
    new_text=re.sub("[^0-9A-Za-z ]", "" , text)
    return new_text
def tokenize_text(text):
    new_text=word_tokenize(text)
    return new_text

import nltk
nltk.download('punkt')
cleaned_text=extra_space(text_corpus1)
print("Removed Extra Spaces")
cleaned_text=sp_charac(cleaned_text)
print("Removed Special Caracters")
cleaned_text=tokenize_text(cleaned_text)

# Storing them in separate text files
import pickle
with open("cleaned_text.txt", "wb") as fp:   #Pickling
    pickle.dump(cleaned_text, fp)

import pickle
with open("cleaned_text.txt", "rb") as fp:   # Unpickling
    cleaned_text = pickle.load(fp)

cleaned_corpus=cleaned_text
print(len(cleaned_corpus))

print(cleaned_corpus[:50])

"""**Creating dictionary of unigrams with stopwords**"""

word_count={}
for word in cleaned_corpus:
    if word not in word_count:
        word_count[word]=0
    word_count[word]+=1

import numpy as np
np.save('unigram_dict.npy', word_count)

freq_df  = pd.DataFrame.from_dict(word_count,orient='index',columns=['Count'])
freq_df=freq_df.sort_values(by=['Count'],ascending=False)
freq_df.head()

"""**Creating dictionary of unigrams without stopwords**"""

from wordcloud import WordCloud, STOPWORDS
counter={}
for i in word_count.keys():
    if i not in list(STOPWORDS):
        counter[i]=word_count[i]
print(len(counter.keys()))

"""**Visualization**"""

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(50,20))
sns.barplot(freq_df.head(50).index,freq_df.head(50)['Count'])
plt.xlabel("Top 50 words")
plt.ylabel("Frequency")
plt.show()

"""Visualising the top 50 words based on their frequency of occurrence
Most of them are stopwords as they are used frequently .
"""

plt.figure(figsize=(100,20))
sns.barplot(freq_df.tail(50).index,freq_df.tail(50)['Count'])
plt.xlabel("Last 50 words")
plt.ylabel("Frequency")
plt.show()

"""**UNIGRAM WORDCLOUD**"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

wordcloud = WordCloud( background_color="black").generate_from_frequencies(counter)
plt.figure(figsize=(8, 5))
plt.axis("off")
plt.title("Unigram Wordcloud")
plt.imshow(wordcloud)

"""**BIGRAM WORDCLOUD**"""

from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder
finder = BigramCollocationFinder.from_words(cleaned_corpus)
bigram_measures = BigramAssocMeasures()
scored = finder.score_ngrams(bigram_measures.raw_freq)
#joining the two words with an _
bigram_dict={}
for i in range(len(scored)):
    bigram_dict['_'.join(scored[i][0])] = scored[i][1]

np.save('bigram_dict.npy', bigram_dict)

import matplotlib.pyplot as plt
wordcloud = WordCloud( background_color="black").generate_from_frequencies(bigram_dict)
plt.figure(figsize=(10, 5))
plt.axis("off")
plt.title("Bigram Wordcloud")
plt.imshow(wordcloud)
plt.show()

"""There is a lost of preposition use as observed from the word cloud"""

del bigram_dict

"""**TRIGRAM WORDCLOUD**"""

data=" ".join(cleaned_corpus[:100000])

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(3,3),stop_words=None)
X = vectorizer.fit_transform([data])
vocab = vectorizer.vocabulary_
print("Vectorized the data : ")
count_values = X.toarray().sum(axis=0)
print("Creating Trigram Dictionary")
trigram_dict={}
for ng_count, ng_text in sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True):
    trigram_dict[ng_text]=ng_count

np.save('trigram_dict.npy', trigram_dict)

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
wordcloud = WordCloud( background_color="black").generate_from_frequencies(trigram_dict)
plt.figure(figsize=(10, 5))
plt.axis("off")
plt.title("Trigram Wordcloud")
plt.imshow(wordcloud)
plt.show()

"""Most commonly used phrases


'one of the'


'out of the'


'be able to'


'some of the'


'going to be'
"""

del trigram_dict

"""**QUADGRAM WORDCLOUD**"""

quad_data=" ".join(cleaned_corpus[:1000000])

vectorizer = CountVectorizer(ngram_range=(4,4),stop_words=None)
X = vectorizer.fit_transform([quad_data])
vocab = vectorizer.vocabulary_
print("Vectorized the data : ")
count_values = X.toarray().sum(axis=0)
print("Creating Quadgram Dictionary")
quadgram_dict={}
for ng_count, ng_text in sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True):
    quadgram_dict[ng_text]=ng_count

np.save('quadgram_dict.npy', quadgram_dict)

wordcloud = WordCloud( background_color="black").generate_from_frequencies(quadgram_dict)
plt.figure(figsize=(10, 5))
plt.axis("off")
plt.title("Quadgram Wordcloud")
plt.imshow(wordcloud)
plt.show()

"""Most commonly used phrases

'in the middle of'

'one of the most'

'the rest of the'
"""

# As these dictionaries take up a huge space , so I have deleted them
del quadgram_dict